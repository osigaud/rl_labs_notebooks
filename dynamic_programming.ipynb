{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic programming functions\n",
    "\n",
    "Dynamic programming functions are used for planning, they require a full knowledge of the MDP from the agent (in contrast to \"true\" RL where the agent does not know the transition and reward functions).\n",
    "\n",
    "The goal of an RL agent is to find the optimal behaviour, defined by a function called policy $\\pi$ that assigns an action (or distribution over actions) to each state so as to maximize the agent's total expected reward. In order to estimate how good a state is, either a state value function $V(x)$ or a state-action value function $Q(x,u)$ is used. $V^{\\pi}(x)$ is the expected return when starting from state $x$ then following policy $\\pi$ and it is processed based on the Bellman Expectation Equation for deterministic policies:\n",
    "$$V^\\pi(x) = r(x, \\pi(x)) + \\gamma \\sum_{y}p(x, \\pi(x), y)V^\\pi(y),$$\n",
    "    \n",
    "where:\n",
    "\n",
    "* $\\pi$ is a deterministic policy, meaning that in a state $x$, the agent always selects the same action,\n",
    "    \n",
    "* $r(x, \\pi(x))$ is the reward gained when taking action based on policy $\\pi$ in state $x$,\n",
    "\n",
    "* $p(x, \\pi(x), y)$ is the probability of ending in state $y$ when taking action under policy $\\pi$ while in state $x$, \n",
    "\n",
    "* $V^\\pi(y)$ is the value of the state $y$ under policy $\\pi$,\n",
    "\n",
    "* $\\gamma \\in [0,1]$ is a discount factor which defines the relative importance of long term rewards over short term ones ( the more it is close to 0 the more the agent's focus is set on immediate rewards).\n",
    "\n",
    "\n",
    "\n",
    "## Value iteration \n",
    "\n",
    "### Value iteration with the *V* function\n",
    "\n",
    "    \n",
    "Value Iteration aims at finding the optimal values $V^*$ based on the Bellman Optimality Equation:\n",
    "$$V^*(x) = \\max_u \\big[ r(x,u) + \\gamma \\sum_{y \\in S} P(x,u,y)V^*(y) \\big].$$\n",
    "\n",
    "The cell below provides the code of value iteration using the $V$ function. It is given as an example from which you will derive other instances of dynamic programming algorithms. Look at it more closely, this will help for later questions:\n",
    "\n",
    "* you can ignore the \"new_render\" and \"render\" functions which are here to provide the visualization of the iterations.\n",
    "\n",
    "* find in the code below the loop over states, the main loop that performs these updates until the values don't change significantly anymore, the main update equation.\n",
    "\n",
    "Found them? OK, go to the next cells..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "########################### Value Iteration ###########################\n",
    "# Given a MDP, this algorithm computes the optimal state value function V\n",
    "# It then derives the optimal policy based on this function\n",
    "\n",
    "def VI_V(mdp, render=True): #Value Iteration using the state value V\n",
    "     \n",
    "    V = np.zeros((mdp.observation_space.size)) #initial state values are set to 0\n",
    "    pol = np.zeros((mdp.observation_space.size)) #initial policy set to always go north\n",
    "    quitt = False\n",
    "    \n",
    "    V_list = [V.copy()] #list of the state values computed over time (used to generate an animation)\n",
    "    policy_list = [pol.copy()] #list of the policies computed over time (used to generate an animation)\n",
    "    \n",
    "    if render:\n",
    "        mdp.new_render()\n",
    "        \n",
    "    while quitt==False:\n",
    "        Vold = V.copy()\n",
    "        if render:\n",
    "            mdp.render(V, pol)\n",
    "        \n",
    "        for x in mdp.observation_space.states : #for each state x\n",
    "            \n",
    "            # Compute the value of the state x for each action u of the MDP action space\n",
    "            V_temp = [] \n",
    "            for u in mdp.action_space.actions : \n",
    "                if not x in mdp.terminal_states:\n",
    "                    # Process sum of the values of the neighbouring states\n",
    "                    sum = 0\n",
    "                    for y in mdp.observation_space.states:\n",
    "                        sum = sum + mdp.P[x,u,y]*Vold[y]\n",
    "                    V_temp.append(mdp.r[x,u] + mdp.gamma*sum) \n",
    "                else : # if the it is one of the final states, then we only take the rewardinto account\n",
    "                    V_temp.append(mdp.r[x,u]) \n",
    "            \n",
    "            # Select the highest state value among those computed\n",
    "            V[x] = np.max(V_temp)\n",
    "            \n",
    "            # Set the policy for this state as the action u that maximizes the state value of x\n",
    "            pol[x] = np.argmax(V_temp)\n",
    "        \n",
    "        V_list.append(V.copy())\n",
    "        policy_list.append(pol.copy())\n",
    "    \n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(V-Vold)) < 0.01 :\n",
    "            quitt = True\n",
    "    \n",
    "    if render:\n",
    "            mdp.render(V, pol)\n",
    "        \n",
    "    \n",
    "    return [V_list, policy_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running and visualizing value iteration\n",
    "\n",
    "The cell below runs a visualization of value iteration. Just run it to see it how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from ipynb.fs.defs.mdp import maze_mdp # Markov Decision Process\n",
    "from ipynb.fs.defs.maze_plotter import maze_plotter # used for visualization of the state value and policy evolution\n",
    "\n",
    "walls = [14,15,16,31,45,46,47]\n",
    "height = 6\n",
    "width = 9\n",
    "m = maze_mdp(width, height, walls=walls, terminal_states=[height*width-1]) # maze-like MDP definition\n",
    "\n",
    "[V_list, policy_list] = VI_V(m,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration with the *Q* function\n",
    "\n",
    "The state-action value function $Q^{\\pi}(s,a)$ defines the value of being in state $x$, taking action $u$ then following policy $\\pi$. The Bellman Optimality Equation for $Q^*$ is:\n",
    "    $$ Q^*(x,u) =  r(x,u) + \\gamma \\sum_{y} P(x,u,y) \\max_{u'}Q^*(y,u'). $$ \n",
    "\n",
    "By taking inspiration from the VI_V(mdp) function given above, fill the blank (given with '//...') in the code below.\n",
    "Run in with the cell given just after the VI_Q(mdp) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def VI_Q(mdp, render=True): #Value Iteration based on the state-action value Q\n",
    "    #Same as above, but we save all the computed values in the Q table \n",
    "    #(instead of saving only the optimal value of each state), so there is no need for a V_temp list\n",
    "    Q = np.zeros((mdp.observation_space.size,mdp.action_space.size))\n",
    "    pol = np.zeros((mdp.observation_space.size))\n",
    "    quitt = False\n",
    "    \n",
    "    Q_list = [Q.copy()] \n",
    "    pol_list = [pol.copy()]\n",
    "    Qmax = Q.max(axis=1)\n",
    "        \n",
    "    if render:\n",
    "        mdp.new_render()\n",
    "\n",
    "        \n",
    "    while quitt==False:\n",
    "        Qold = Q.copy()\n",
    "        \n",
    "        if render:\n",
    "            mdp.render(Q,pol)\n",
    "        \n",
    "        \n",
    "        for x in mdp.observation_space.states :\n",
    "            for u in mdp.action_space.actions :\n",
    "                Q[x,u] = \"\\\\...\"\n",
    "                \n",
    "        Qmax = Q.max(axis=1)\n",
    "        pol =  np.argmax(Q,axis=1)\n",
    "        \n",
    "        Q_list.append(Q.copy())\n",
    "        pol_list.append(pol)\n",
    "        \n",
    "       \n",
    "        if (np.linalg.norm(Q-Qold)) <= 0.01 :\n",
    "            quitt = True\n",
    "    \n",
    "    if render:\n",
    "        mdp.render(Q,pol)\n",
    "        \n",
    "    return [Q_list, pol_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\\\... run your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "### Policy Iteration with the *Q* function\n",
    "\n",
    "Given a MDP and a policy $\\pi$, policy iteration goes through the following process: \n",
    "* Evaluation of the policy $\\pi$: computes the state-action value $Q$ based on the policy $\\pi$;\n",
    "* Improvement of the policy $\\pi$: computes the new best policy based on the state-action value $Q$.\n",
    "\n",
    "This process is repeated until convergence is achieved (the policy cannot be improved anymore).\n",
    "\n",
    "The evaluation process relies on the Bellman Expectation Equation for $Q$ with deterministic policy $\\pi$: \n",
    "    $$Q^{\\pi}(x,u) = r(x,u) + \\gamma \\sum_{y \\in S}P(x,u,y)Q^{\\pi}(y,\\pi(y)).$$\n",
    "\n",
    "The policy can then be updated as follows:\n",
    "    $$\\pi^{(t+1)}(x) = \\arg\\max_uQ^{\\pi^{(t)}}(x,u).$$\n",
    "\n",
    "The box below gives the code of policy iteration with the $Q$ function. Again, have a close look at the code.\n",
    "\n",
    "* can you find the part where the policy is evaluated, and the part where it is updated? Why do we have to loop twice over states in the former?\n",
    "\n",
    "In the cell given just after the PI_Q(mdp) function, by taking inspiration of the cell just above, write the code to run it and vizualize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def PI_Q(mdp, render=True): # policy iteration over the Q function\n",
    "    Q = np.zeros((mdp.observation_space.size,mdp.action_space.size))\n",
    "    pol = np.zeros(mdp.observation_space.size,dtype=np.int16)\n",
    "    \n",
    "    Q_list = [Q.copy()]\n",
    "    policy_list = [pol.copy()]\n",
    "    \n",
    "    quitt = False\n",
    "    \n",
    "    if render:\n",
    "        mdp.new_render()\n",
    "    \n",
    "    while quitt==False:\n",
    "\n",
    "        if render:\n",
    "            mdp.render(Q.max(axis=1),pol)\n",
    "            \n",
    "        # Step 1 : Policy Evaluation\n",
    "        for x in mdp.observation_space.states:\n",
    "            for u in mdp.action_space.actions:\n",
    "                if x in mdp.terminal_states:\n",
    "                    Q[x,u] = mdp.r[x,u]\n",
    "                else :\n",
    "                    sum = 0\n",
    "                    for y in mdp.observation_space.states:\n",
    "                        sum = sum + mdp.P[x,u,y]*Q[y,pol[y]]\n",
    "                    Q[x,u] = mdp.r[x,u] + mdp.gamma*sum\n",
    "        \n",
    "            \n",
    "        # Step 2 : Policy Improvement\n",
    "        pol =  np.argmax(Q,axis=1)         \n",
    "        \n",
    "        Q_list.append(Q.copy())\n",
    "        policy_list.append(pol.copy())\n",
    "        \n",
    "        # Check the convergence\n",
    "        if np.array_equal(Q_list[-1],Q_list[-2]) :\n",
    "            quitt = True\n",
    "    \n",
    "    if render:\n",
    "            mdp.render(Q.max(axis=1),pol)\n",
    "            \n",
    "    return [Q_list,policy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\\\... run the above code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration with the *V* function\n",
    "\n",
    "By taking inspiration from the PI_Q(mdp) function given above, write a PI_V(mdp) function that implements policy iteration with the *V* function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\\\ write the PI_V(mdp) function here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\\\ run the PI_V(mdp) function here to visualize it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "Which of all the above methods converges in fewer steps? To answer the question, add a step counter to the above codes, run them again and print the step counter in the end.\n",
    "\n",
    "Discuss the relative computational efficiency of these methods.\n",
    "\n",
    "### Open question\n",
    "\n",
    "The [*Generalized Policy Iteration*](https://link.springer.com/content/pdf/10.1007/BF00933260.pdf) algorithm draws a continuum between value iteration and policy iteration. Try to implement it and parametrize it to obtain the value iteration regime, the policy iteration regime, and something intermediate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
